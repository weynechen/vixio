# Vixio Provider Configuration
# Supports multiple deployment modes: dev, dev-local-cn, dev-qwen, dev-realtime, docker, k8s

# ==============================================================================
# Development Mode - Qwen Cloud API (Alibaba DashScope)
# ==============================================================================
# Uses Alibaba Cloud's Qwen models for ASR and TTS via DashScope WebSocket API.
# Local VAD for voice activity detection.
# 
# Usage:
#   pip install vixio[dev-qwen]  # or: uv pip install -e ".[dev-qwen]"
#   export DASHSCOPE_API_KEY="your-api-key"
#   uv run python examples/agent_chat.py --env dev-qwen-pipeline
# ==============================================================================
dev-qwen-pipeline:
  providers:
    # VAD Provider (In-Process - Silero ONNX)
    vad:
      provider: silero-vad-local
      config:
        threshold: 0.35
        threshold_low: 0.15
        frame_window_threshold: 8
        use_gpu: false
    
    # ASR Provider (Qwen Cloud - Realtime WebSocket)
    asr:
      provider: qwen3-asr-flash-realtime
      config:
        api_key: ${DASHSCOPE_API_KEY}
        model: "qwen3-asr-flash-realtime"
        language: "zh"
        sample_rate: 16000
        enable_vad: false  # Use external VAD (Silero above)
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
        mcp_server:
           name: "gaode-map"
           url: ${MCP_SERVER_URL}
           transport: "sse"  # or "streamablehttp"
    
    # TTS Provider (Qwen Cloud - Realtime WebSocket)
    tts:
      provider: qwen3-tts-flash-realtime
      config:
        api_key: ${DASHSCOPE_API_KEY}
        model: "qwen3-tts-flash-realtime"
        voice: "Cherry"
        mode: "commit"  # use manual mode
        language_type: "Chinese"

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"


# ==============================================================================
# Development Mode - Qwen Full Streaming (Continuous Audio/Text)
# ==============================================================================
# Full streaming pipeline with continuous audio/text streaming.
# Uses Qwen's ASR built-in VAD and TTS server_commit mode.
# 
# Pipeline: StreamingASR(VAD) → Agent → StreamingTTS
# Benefits: Minimal latency (~500-1000ms TTFT), simplest pipeline (4 stations)
# Trade-offs: Requires stable network, ASR VAD may be less accurate
# 
# Usage:
#   pip install vixio[dev-qwen]  # or: uv pip install -e ".[dev-qwen]"
#   export DASHSCOPE_API_KEY="your-api-key"
#   uv run python examples/agent_chat_streaming.py
# ==============================================================================
dev-qwen-streaming:
  providers:
    # No VAD provider needed! ASR has built-in VAD
    
    # ASR Provider (Qwen Cloud - Continuous streaming with built-in VAD)
    asr:
      provider: qwen3-asr-flash-realtime
      config:
        api_key: ${DASHSCOPE_API_KEY}
        model: "qwen3-asr-flash-realtime"
        language: "zh"
        sample_rate: 16000
        enable_vad: true  # Built-in VAD for streaming mode
        vad_threshold: 0.5
        silence_duration_ms: 800
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (Qwen Cloud - Streaming input with auto-segmentation)
    tts:
      provider: qwen3-tts-flash-realtime
      config:
        api_key: ${DASHSCOPE_API_KEY}
        model: "qwen3-tts-flash-realtime"
        voice: "Cherry"
        mode: "server_commit"  # Streaming input + auto-segmentation
        language_type: "Chinese"

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"

# ==============================================================================
# Development Mode - Qwen Omni Realtime (End-to-End Voice)
# ==============================================================================
# Uses Alibaba Cloud's Qwen-Omni-Realtime model for end-to-end voice conversation.
# Integrates VAD + ASR + LLM + TTS in a single model.
# No separate VAD, ASR, Agent, or TTS providers needed.
# 
# Usage:
#   pip install vixio[dev-qwen]  # or: uv pip install -e ".[dev-qwen]"
#   export DASHSCOPE_API_KEY="your-api-key"
#   uv run python examples/realtime_chat.py --env dev-realtime
# ==============================================================================
dev-qwen-realtime:
  providers:
    # Realtime Provider (Qwen Omni - End-to-End Voice)
    realtime:
      provider: qwen-omni-realtime
      config:
        api_key: ${DASHSCOPE_API_KEY}
        model: "qwen3-omni-flash-realtime"
        voice: "Cherry"
        instructions: "你是一个友好的AI语音助手，请用简洁自然的语言回答问题。回答要口语化，适合语音播放。"
        input_sample_rate: 16000
        output_sample_rate: 24000
        vad_threshold: 0.5
        silence_duration_ms: 800

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"

# ==============================================================================
# Development Mode - In-Process (No External Services)
# ==============================================================================
# All inference runs in the same process. No Docker or gRPC services needed.
# Requires installing the local inference dependencies.
# 
# Usage:
#   pip install vixio[dev-local-cn]  # or: uv pip install -e ".[dev-local-cn]"
#   uv run python examples/agent_chat.py --env dev-local-cn
# ==============================================================================
dev-in-process:
  providers:
    # VAD Provider (In-Process - ONNX Runtime)
    vad:
      provider: silero-vad-local
      config:
        threshold: 0.35
        threshold_low: 0.15
        frame_window_threshold: 8
        use_gpu: true
    
    # ASR Provider (In-Process - Sherpa ONNX)
    asr:
      provider: sherpa-onnx-asr-local
      config:
        model_path: "models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17"
        language: "auto"
        num_threads: 4
        use_itn: true
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (In-Process - Kokoro)
    tts:
      provider: kokoro-tts-local
      config:
        voice: "zf_001"
        speed: 1.1
        lang: "zh"
        sample_rate: 16000
        repo_id: "hexgrad/Kokoro-82M-v1.1-zh"

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"

# ==============================================================================
# Development Mode - gRPC Services
# ==============================================================================
# Requires running gRPC services separately (Docker or local processes).
# - VAD: localhost:50051 (1 replica)
# - ASR: localhost:50052 (1 replica)
# - TTS: localhost:50053 (1 replica)
# 
# Usage:
#   docker-compose up -d vad asr tts  # Start services
#   uv run python examples/agent_chat.py --env dev
# ==============================================================================
dev-grpc:
  providers:
    # VAD Provider (Local gRPC)
    vad:
      provider: silero-vad-grpc
      config:
        service_url: "localhost:50051"
        # Optimized for handling sentence pauses (300-800ms):
        threshold: 0.35              # Lower = more sensitive (detect weak voice during pauses)
        threshold_low: 0.15          # Lower = wider hysteresis range
        frame_window_threshold: 8    # Higher = need more silent frames to confirm end (was 3)
    
    # ASR Provider (Local gRPC)
    asr:
      provider: sherpa-onnx-asr-grpc
      config:
        service_url: "localhost:50052"
        language: "auto"
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL}
        base_url: ${BASE_URL}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (可选: edge-tts-remote 或 kokoro-tts-grpc)
    # tts:
    #   provider: edge-tts-remote
    #   config:
    #     voice: "zh-CN-XiaoxiaoNeural"
    #     rate: "+0%"
    #     volume: "+0%"
    
    # TTS Provider (Local gRPC - Kokoro)
    tts:
      provider: kokoro-tts-grpc
      config:
        service_url: "localhost:50053"
        voice: "zf_001"
        speed: 1.1          # 稍微加快语速
        lang: "zh"
        sample_rate: 24000  # Kokoro 原生采样率

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"
      

# ==============================================================================
# Docker Mode
# ==============================================================================
# Docker containers with Docker Compose
# - VAD: vad-service:50051 (1 replica)
# - ASR: asr-service:50052 (1 replica)
# 
# Usage:
#   docker-compose up -d
# ==============================================================================
docker:
  providers:
    # VAD Provider (Local gRPC via Docker DNS)
    vad:
      provider: silero-vad-grpc
      config:
        service_url: "silero-vad-service:50051"
        # Optimized for handling sentence pauses (300-800ms):
        threshold: 0.35              # Lower = more sensitive (detect weak voice during pauses)
        threshold_low: 0.15          # Lower = wider hysteresis range
        frame_window_threshold: 8    # Higher = need more silent frames to confirm end (was 3)
    
    # ASR Provider (Local gRPC via Docker DNS)
    asr:
      provider: sherpa-onnx-asr-grpc
      config:
        service_url: "sherpa-asr-service:50052"
        language: "auto"
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent-remote
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (Remote API)
    tts:
      provider: edge-tts-remote
      config:
        voice: "zh-CN-XiaoxiaoNeural"
        rate: "+0%"
        volume: "+0%"

# ==============================================================================
# Kubernetes Mode
# ==============================================================================
# K8s with HPA (Horizontal Pod Autoscaling)
# - VAD: vad-service:50051 (2-10 replicas, auto-scaled)
# - ASR: asr-service:50052 (2-10 replicas, auto-scaled)
# 
# Usage:
#   kubectl apply -f k8s/
# ==============================================================================
k8s:
  providers:
    # VAD Provider (Local gRPC via K8s Service)
    vad:
      provider: silero-vad-grpc
      config:
        service_url: "silero-vad-service:50051"
        # Optimized for handling sentence pauses (300-800ms):
        threshold: 0.35              # Lower = more sensitive (detect weak voice during pauses)
        threshold_low: 0.15          # Lower = wider hysteresis range
        frame_window_threshold: 8    # Higher = need more silent frames to confirm end (was 3)
    
    # ASR Provider (Local gRPC via K8s Service)
    asr:
      provider: sherpa-onnx-asr-grpc
      config:
        service_url: "sherpa-asr-service:50052"
        language: "auto"
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent-remote
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (Remote API)
    tts:
      provider: edge-tts-remote
      config:
        voice: "zh-CN-XiaoxiaoNeural"
        rate: "+0%"
        volume: "+0%"

# ==============================================================================
# Notes:
# ==============================================================================
# 1. Environment Variables:
#    - Use ${VAR_NAME} syntax for environment variable substitution
#    - Example: api_key: ${OPENAI_API_KEY}
# 
# 2. Provider Types:
#    - Local (is_local=True): Self-hosted services (VAD, ASR)
#      - Requires our own deployment and scaling
#      - Uses gRPC for inter-service communication
#    - Remote (is_local=False): Cloud APIs (OpenAI, Edge TTS)
#      - Scaling handled by cloud provider
#      - Uses HTTP/REST APIs
# 
# 3. Deployment Differences:
#    - Dev: Single-instance services on localhost
#    - Docker: Single-instance services with Docker DNS
#    - K8s: Multi-replica services with K8s load balancing
# 
# 4. Concurrency Model:
#    - Stateful providers (VAD): VAD-cycle locking (START→END)
#    - Stateless providers (ASR): Chunk-level concurrency
#    - K8s HPA: Auto-scale based on CPU utilization
#
# 5. MCP Server Configuration (Optional):
#    Agent provider supports hosted MCP servers for tool expansion.
#    
#    Single server (simplified):
#      agent:
#        provider: openai-agent
#        config:
#          api_key: ${API_KEY}
#          model: ${LITELLM_MODEL}
#          mcp_server:
#            name: "gaode-map"
#            url: ${MCP_SERVER_URL}
#            transport: "sse"  # or "streamablehttp"
#    
#    Multiple servers:
#      agent:
#        provider: openai-agent
#        config:
#          api_key: ${API_KEY}
#          model: ${LITELLM_MODEL}
#          mcp_servers:
#            - name: "gaode-map"
#              url: ${MCP_GAODE_URL}
#              transport: "sse"
#            - name: "weather"
#              url: ${MCP_WEATHER_URL}
#              transport: "streamablehttp"
#    
#    Environment variables example:
#      MCP_SERVER_URL=https://mcp.api-inference.modelscope.net/xxxxx/sse
#      MCP_GAODE_URL=https://mcp.api-inference.modelscope.net/gaode/sse
#      MCP_WEATHER_URL=http://localhost:8000/mcp
# ==============================================================================

