# Vixio Provider Configuration
# Supports multiple deployment modes: dev, dev-local-cn, docker, k8s

# ==============================================================================
# Development Mode - In-Process (No External Services)
# ==============================================================================
# All inference runs in the same process. No Docker or gRPC services needed.
# Requires installing the local inference dependencies.
# 
# Usage:
#   pip install vixio[dev-local-cn]  # or: uv pip install -e ".[dev-local-cn]"
#   uv run python examples/agent_chat.py --env dev-local-cn
# ==============================================================================
dev-local-cn:
  providers:
    # VAD Provider (In-Process - ONNX Runtime)
    vad:
      provider: silero-vad-local
      config:
        threshold: 0.35
        threshold_low: 0.15
        frame_window_threshold: 8
        use_gpu: true
    
    # ASR Provider (In-Process - Sherpa ONNX)
    asr:
      provider: sherpa-onnx-asr-local
      config:
        model_path: "models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17"
        language: "auto"
        num_threads: 4
        use_itn: true
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (In-Process - Kokoro)
    tts:
      provider: kokoro-tts-local
      config:
        voice: "zf_001"
        speed: 1.1
        lang: "zh"
        sample_rate: 16000
        repo_id: "hexgrad/Kokoro-82M-v1.1-zh"

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"

# ==============================================================================
# Development Mode - gRPC Services
# ==============================================================================
# Requires running gRPC services separately (Docker or local processes).
# - VAD: localhost:50051 (1 replica)
# - ASR: localhost:50052 (1 replica)
# - TTS: localhost:50053 (1 replica)
# 
# Usage:
#   docker-compose up -d vad asr tts  # Start services
#   uv run python examples/agent_chat.py --env dev
# ==============================================================================
dev:
  providers:
    # VAD Provider (Local gRPC)
    vad:
      provider: silero-vad-grpc
      config:
        service_url: "localhost:50051"
        # Optimized for handling sentence pauses (300-800ms):
        threshold: 0.35              # Lower = more sensitive (detect weak voice during pauses)
        threshold_low: 0.15          # Lower = wider hysteresis range
        frame_window_threshold: 8    # Higher = need more silent frames to confirm end (was 3)
    
    # ASR Provider (Local gRPC)
    asr:
      provider: sherpa-onnx-asr-grpc
      config:
        service_url: "localhost:50052"
        language: "auto"
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent-remote
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL}
        base_url: ${BASE_URL}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (可选: edge-tts-remote 或 kokoro-tts-grpc)
    # tts:
    #   provider: edge-tts-remote
    #   config:
    #     voice: "zh-CN-XiaoxiaoNeural"
    #     rate: "+0%"
    #     volume: "+0%"
    
    # TTS Provider (Local gRPC - Kokoro)
    tts:
      provider: kokoro-tts-grpc
      config:
        service_url: "localhost:50053"
        voice: "zf_001"
        speed: 1.1          # 稍微加快语速
        lang: "zh"
        sample_rate: 24000  # Kokoro 原生采样率

    vlm:
      provider: openai-vlm-remote
      config:
        model: "glm-4v-flash"
        api_key: "${GLM_API_KEY}"
        base_url: "https://open.bigmodel.cn/api/paas/v4"
      

# ==============================================================================
# Docker Mode
# ==============================================================================
# Docker containers with Docker Compose
# - VAD: vad-service:50051 (1 replica)
# - ASR: asr-service:50052 (1 replica)
# 
# Usage:
#   docker-compose up -d
# ==============================================================================
docker:
  providers:
    # VAD Provider (Local gRPC via Docker DNS)
    vad:
      provider: silero-vad-grpc
      config:
        service_url: "silero-vad-service:50051"
        # Optimized for handling sentence pauses (300-800ms):
        threshold: 0.35              # Lower = more sensitive (detect weak voice during pauses)
        threshold_low: 0.15          # Lower = wider hysteresis range
        frame_window_threshold: 8    # Higher = need more silent frames to confirm end (was 3)
    
    # ASR Provider (Local gRPC via Docker DNS)
    asr:
      provider: sherpa-onnx-asr-grpc
      config:
        service_url: "sherpa-asr-service:50052"
        language: "auto"
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent-remote
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (Remote API)
    tts:
      provider: edge-tts-remote
      config:
        voice: "zh-CN-XiaoxiaoNeural"
        rate: "+0%"
        volume: "+0%"

# ==============================================================================
# Kubernetes Mode
# ==============================================================================
# K8s with HPA (Horizontal Pod Autoscaling)
# - VAD: vad-service:50051 (2-10 replicas, auto-scaled)
# - ASR: asr-service:50052 (2-10 replicas, auto-scaled)
# 
# Usage:
#   kubectl apply -f k8s/
# ==============================================================================
k8s:
  providers:
    # VAD Provider (Local gRPC via K8s Service)
    vad:
      provider: silero-vad-grpc
      config:
        service_url: "silero-vad-service:50051"
        # Optimized for handling sentence pauses (300-800ms):
        threshold: 0.35              # Lower = more sensitive (detect weak voice during pauses)
        threshold_low: 0.15          # Lower = wider hysteresis range
        frame_window_threshold: 8    # Higher = need more silent frames to confirm end (was 3)
    
    # ASR Provider (Local gRPC via K8s Service)
    asr:
      provider: sherpa-onnx-asr-grpc
      config:
        service_url: "sherpa-asr-service:50052"
        language: "auto"
    
    # Agent Provider (Remote API)
    agent:
      provider: openai-agent-remote
      config:
        api_key: ${API_KEY}
        model: ${LITELLM_MODEL:deepseek/deepseek-chat}
        base_url: ${BASE_URL:}
        temperature: 0.7
        max_tokens: 2000
    
    # TTS Provider (Remote API)
    tts:
      provider: edge-tts-remote
      config:
        voice: "zh-CN-XiaoxiaoNeural"
        rate: "+0%"
        volume: "+0%"

# ==============================================================================
# Notes:
# ==============================================================================
# 1. Environment Variables:
#    - Use ${VAR_NAME} syntax for environment variable substitution
#    - Example: api_key: ${OPENAI_API_KEY}
# 
# 2. Provider Types:
#    - Local (is_local=True): Self-hosted services (VAD, ASR)
#      - Requires our own deployment and scaling
#      - Uses gRPC for inter-service communication
#    - Remote (is_local=False): Cloud APIs (OpenAI, Edge TTS)
#      - Scaling handled by cloud provider
#      - Uses HTTP/REST APIs
# 
# 3. Deployment Differences:
#    - Dev: Single-instance services on localhost
#    - Docker: Single-instance services with Docker DNS
#    - K8s: Multi-replica services with K8s load balancing
# 
# 4. Concurrency Model:
#    - Stateful providers (VAD): VAD-cycle locking (START→END)
#    - Stateless providers (ASR): Chunk-level concurrency
#    - K8s HPA: Auto-scale based on CPU utilization
# ==============================================================================

