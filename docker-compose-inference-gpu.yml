# Docker Compose for Vixio Services (GPU version)
# Requires NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html

version: '3.8'

services:
  # Silero VAD gRPC Service (GPU)
  silero-vad-service:
    build:
      context: .
      dockerfile: inference/silero_vad/Dockerfile.gpu
    container_name: vixio-silero-vad-service-gpu
    ports:
      - "50051:50051"
    environment:
      - LOG_LEVEL=INFO
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.connect((\"localhost\", 50051)); s.close()'"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    networks:
      - vixio-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 300M
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  # Sherpa ONNX ASR gRPC Service (GPU)
  sherpa-asr-service:
    build:
      context: .
      dockerfile: inference/sherpa_onnx_local/Dockerfile.gpu
    container_name: vixio-sherpa-asr-service-gpu
    ports:
      - "50052:50052"
    volumes:
      - ./models:/app/models:ro  # Mount models directory (read-only)
    environment:
      - LOG_LEVEL=INFO
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.connect((\"localhost\", 50052)); s.close()'"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    networks:
      - vixio-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 500M
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  # Kokoro TTS gRPC Service (GPU)
  kokoro-tts-service:
    build:
      context: .
      dockerfile: inference/kokoro_cn_tts_local/Dockerfile.gpu
    container_name: vixio-kokoro-tts-service-gpu
    ports:
      - "50053:50053"
    environment:
      - LOG_LEVEL=INFO
      - NVIDIA_VISIBLE_DEVICES=all
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import socket; s=socket.socket(); s.connect((\"localhost\", 50053)); s.close()'"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - vixio-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

networks:
  vixio-network:
    driver: bridge

# Usage:
# ------
# Prerequisites:
#   1. Install NVIDIA drivers
#   2. Install NVIDIA Container Toolkit:
#      https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
#
# Start services:
#   docker-compose -f docker-compose-inference-gpu.yml up -d --build
#
# For China users (with PyPI mirror):
#   docker-compose -f docker-compose-inference-gpu.yml -f docker-compose-inference-gpu-cn.yml up -d --build
#
# View logs:
#   docker-compose -f docker-compose-inference-gpu.yml logs -f
#
# Stop services:
#   docker-compose -f docker-compose-inference-gpu.yml down

